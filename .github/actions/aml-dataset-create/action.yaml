name: 'Generating datasets'
description: 'Ensure that a given dataset exists in Azure Machine Learning Services. If the dataset doesnt exit, it is created and can be initialized with data which will be uploaded to Azure Storage accounts.'

inputs:
    datasetFile:
      description: 'Dataset YAML definition file. Wildcard paths are supported.'
      required: true
    workspaceName:
      description: 'Name of the workspace to work against.'
      required: true
    resourceGroup:
      description: 'Name of the resource group where the workspace is placed.'
      required: true
    initialize:
      description: 'Indicates if the dataset should be initialized with same data in the current repository.'
      required: true
      default: 'false'
    initialDataPath:
      description: 'Path where the data is located. This path is relative to the location of the dataset YAML definition file. Required if `initialize` is set to `true`.'
      required: false
      default: 'data'
    storageAccount:
      description: 'Name of the storage account where data should be uploaded. This storage account should be also registered in Azure Machine Learning as a data store. Required if `initialize` is set to `true`.'
      required: false

runs:
  using: "composite"
  steps:
    - name: Generating datasets
      shell: bash
      run: |
        echo "::debug::Looking for datasets definition at '${{ inputs.datasetFile }}'"
        DATASETS_FILES=$(find ${{ inputs.datasetFile }};)

        for DATASET_FILE in $DATASETS_FILES
        do
          echo "::debug::Working with dataset '$DATASET_FILE'"
          
          DATASET_NAME=$(yq -r ".name" $DATASET_FILE)
          REMOTE_FILE=$(yq -r ".path" $DATASET_FILE)
          REMOTE_FOLDER=$(dirname $REMOTE_FILE | cut -d/ -f6-)
          CONTAINER_NAME=$(dirname $REMOTE_FILE | cut -d/ -f4)
          LOCAL_FOLDER=$(dirname $DATASET_FILE)

          if [[ $(az ml data list --name $DATASET_NAME --workspace-name ${{ inputs.workspaceName }} --resource-group ${{ inputs.resourceGroup }}) ]]; then
            echo "::debug::Dataset $DATASET_NAME already in target workspace."
          else
            echo "::debug::Dataset $DATASET_NAME is missing. Creating from file $DATASET_FILE."
            az ml data create --file $DATASET_FILE --resource-group ${{ inputs.resourceGroup }} --workspace-name ${{ inputs.workspaceName }}
            
            if ${{ inputs.initialize }}; then
              # Data uploaded manually as AzureFileCopy@4 not supported in Linux
              echo "::debug::Uploading data for $DATASET_NAME in container $CONTAINER_NAME (${{ inputs.storageAccount }})"
              echo "::debug::Source: $LOCAL_FOLDER/${{ inputs.initialDataPath }}"
              echo "::debug::Destination: $REMOTE_FOLDER"

              if test -d "$LOCAL_FOLDER/${{ inputs.initialDataPath }}"; then
                az storage blob upload-batch -d $CONTAINER_NAME --auth-mode login --overwrite --account-name ${{ inputs.storageAccount }} --source "$LOCAL_FOLDER/${{ inputs.initialDataPath }}" --destination-path $REMOTE_FOLDER
              else
                echo "::error::Folder $LOCAL_FOLDER/${{ inputs.initialDataPath }} not found."
                exit 1
              fi
            fi
          fi
        done

